{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验：逻辑回归文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验概要\n",
    "\n",
    "#### 监督式学习\n",
    "\n",
    "与非监督学习不同，监督学习算法需要标记数据 (监督者)。他们学习如何通过分析所提供数据的各种特征，来自动生成标签或预测值。例如：假设你已经在手机上标记了重要的短信，你想要以后每天手机自动检查所有的短信并将重要的信息标记出来。这是监督学习的一个用例。在这里，以前已经加星号的短信，可以作为标记数据使用。使用这些数据，您可以创建两种类型的模型，它们能够实现以下功能:\n",
    "\n",
    "- 对接收到的新消息是否属于重要信息进行分类\n",
    "- 预测未来接收到新的重要信息的可能性\n",
    "\n",
    "第一种类型称为分类，而第二种类型称为回归。其中，分类算法是从给定数据集学习模式来确定未知数据集的类别。一些最广泛使用的分类算法是：逻辑回归、朴素贝叶斯、K 近邻和树方法。我们将在后续的实验中逐个实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逻辑回归\n",
    "\n",
    "逻辑回归（Logistic Regression）是机器学习中的一种分类模型，尽管它的名称中有 **回归** 一词，但逻辑回归被用于概率分类。由于算法的简单和高效，在实际中应用非常广泛。在介绍逻辑回归模型之前，我们先引入 $sigmoid$ 函数，其数学形式是：\n",
    "\n",
    "$$g(x) = \\frac{1}{1 + e ^ {-x}}$$\n",
    "\n",
    "对应的函数曲线如下图所示，我们可以直接用代码绘制出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np                  # 导入numpy科学运算库\n",
    "import matplotlib.pyplot as plt     # 导入matplotlib绘图库\n",
    "\n",
    "# 魔法指令，让matplotlib图形直接在jupyter notebook中显示\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))             # 直接返回sigmoid函数\n",
    " \n",
    "sigmoid_inputs = np.arange(-10,10,0.1)    # 参数: 起点，终点，间距\n",
    "sigmoid_outputs = sigmoid(sigmoid_inputs)\n",
    " \n",
    "plt.plot(sigmoid_inputs,sigmoid_outputs)\n",
    "plt.xlabel(\"Sigmoid Inputs\")\n",
    "plt.ylabel(\"Sigmoid Outputs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看到 sigmoid 函数是一个 $S$ 形的曲线，它的取值在 `[0,1]` 之间，在远离 0 的地方函数的值会很快接近 0/1。这个性质使我们能够以概率的方式来解释。逻辑回归返回的是概率。一方面，你可以“原样”使用返回的概率（例如：用户点击此广告的概率为 0.00023），另一方面，也可以将返回的概率转换成二元值 `0` / `1`（例如，这封电子邮件是垃圾邮件）。对于垃圾邮件分类而言，因变量 (结果) 是二进制的，这意味着值可以用 `0` 或 `1` 表示。在这里，如果邮件是垃圾邮件，那么决策 (因变量或结果) 的值可以被认为是 `1`；否则，它就是 `0`。没有其他结果是可能的。自变量 (即特性) 将由电子邮件的各种属性组成，如某些关键字出现的次数等。然后我们可以使用逻辑回归算法创建一个模型来预测该邮件是否是垃圾邮件(`1`)或非垃圾邮件(`0`)。在这里，决策边界是通过训练一个逻辑回归模型来创建的，该模型可以帮助我们对垃圾邮件进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策函数\n",
    "\n",
    "一个机器学习的模型，实际上是把决策函数限定在某一组条件下，这组限定条件就决定了模型的假设空间。当然，我们还希望这组限定条件简单而合理。而逻辑回归模型所做的假设是：\n",
    "\n",
    "$$P(y=1|x;\\theta) = g(\\theta^T x) = \\frac{1}{1 + e ^ {-\\theta^T * x}}$$\n",
    "\n",
    "这里的 $g(h)$ 是上边提到的 $sigmoid$ 函数，相应的决策函数为：\n",
    "\n",
    "$$y^* = 1, \\, \\textrm{if} \\, P(y=1|x) > 0.5$$\n",
    "\n",
    "譬如：如果某个逻辑回归模型对某封电子邮件进行预测时返回的概率为 0.9995，则表示该模型预测这封邮件非常可能是垃圾邮件。相反，在同一个逻辑回归模型中预测分数为 0.0003 的另一封电子邮件很可能不是垃圾邮件。可如果某封电子邮件的预测分数为 0.6 呢？为了将逻辑回归值映射到二元类别，你必须指定 **分类阈值**（也称为判定阈值）。如果值高于该阈值，则表示 `垃圾邮件`；如果值低于该阈值，则表示 `非垃圾邮件`。人们往往会认为分类阈值应始终为 **0.5**。实际应用时，特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数求解\n",
    "\n",
    "模型的数学形式确定后，剩下就是如何去求解模型中的参数。统计学中常用的一种方法是最大似然估计，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）越大。在逻辑回归模型中，似然度可表示为：\n",
    "\n",
    "$$L(\\theta) = P(D|\\theta) = \\prod P(y|x;\\theta) = \\prod g(\\theta^T x) ^ y (1-g(\\theta^T x))^{1-y}$$\n",
    "\n",
    "取对数可以得到对数似然度：\n",
    "\n",
    "$$l(\\theta) = \\sum {y\\log{g(\\theta^T x)} + (1-y)\\log{(1-g(\\theta^T x))}}$$\n",
    "\n",
    "另一方面，在机器学习领域，我们更经常遇到的是损失函数的概念，其衡量的是模型预测错误的程度。常用的损失函数有 `0-1` 损失，$log$ 损失，$hinge$ 损失等。其中 $log$ 损失在单个数据点上的定义为：\n",
    "\n",
    "$$-y\\log{p(y|x)}-(1-y)\\log{1-p(y|x)}$$\n",
    "\n",
    "如果取整个数据集上的平均 $log$ 损失，我们可以得到\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{N} l(\\theta)$$\n",
    "\n",
    "即在逻辑回归模型中，我们最大化似然函数和最小化 $log$ 损失函数实际上是等价的。对于该优化问题，存在多种求解方法，这里以梯度下降的为例说明。**梯度下降** (Gradient Descent) 又叫作：最速梯度下降，是一种迭代求解的方法。通过在每一步选取使目标函数变化最快的一个方向调整参数的值来逼近最优值。\n",
    "\n",
    "基本步骤如下：\n",
    "- 选择下降方向（梯度方向，$\\nabla {J(\\theta)}$）\n",
    "- 选择步长，更新参数：$\\theta^i = \\theta^{i-1} - \\alpha^i \\nabla {J(\\theta^{i-1})}$\n",
    "- 重复以上两步直到满足终止条件\n",
    "\n",
    "其中损失函数的梯度计算方法为：\n",
    "\n",
    "$$\\frac{\\partial{J}}{\\partial{\\theta}} = -\\frac{1}{n}\\sum_i (y_i - y_i^*)x_i + \\lambda \\theta$$\n",
    "\n",
    "沿梯度负方向选择一个较小的步长可以保证损失函数是减小的，另一方面，逻辑回归的损失函数是凸函数（加入正则项后是严格凸函数），可以保证我们找到的局部最优值同时是全局最优。此外，常用的凸优化的方法都可以用于求解该问题。例如：共轭梯度下降，牛顿法，LBFGS 等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验目标\n",
    "\n",
    "在本实验中，我们将使用 logistic 回归分类算法，对 Amazon 上的乐器商品评论进行分类。\n",
    "\n",
    "<img src=\"./img/musical instruments.jpg\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 nltk_data 路径环境变量\n",
    "from nltk import data\n",
    "data.path.append(r'../nltk_data')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import string\n",
    "from nltk import word_tokenize  #分词\n",
    "from nltk.stem import WordNetLemmatizer  #词干提取\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  #计算tf-idf\n",
    "from collections import Counter  #词频统计\n",
    "from pylab import *\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 加载数据\n",
    "\n",
    "实验数据集存储在 `./data/reviews_Musical_Instruments_5.json`，注意你应该以实际存储路径为准："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  Not much to write about here, but it does exac...        5\n",
       "1  The product does exactly as it should and is q...        5\n",
       "2  The primary job of this device is to block the...        5\n",
       "3  Nice windscreen protects my MXL mic and preven...        5\n",
       "4  This pop filter is great. It looks and perform...        5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data = pd.read_json('./data/reviews_Musical_Instruments_5.json', lines=True)\n",
    "review_data[['reviewText', 'overall']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 数据清洗\n",
    "\n",
    "数据清洗主要分为两步：\n",
    "1. 使用正则表达式对文本进行分割并过滤一些特殊符号\n",
    "2. 对分词进行小写变换并进行词干提取 (词形还原)\n",
    "\n",
    "使用 `lambda` 函数从 DataFrame 的每个 `reviewText` 中提取标记，执行词形还原，并将它们并排连接起来。使用 `join` 函数将单词列表连接成一个句子。使用正则表达式方法 `re` 将除字母字符、数字和空格外的任何字符替换为空白。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "review_data['cleaned_review_text'] = review_data['reviewText'].apply(\\\n",
    "lambda x : ' '.join([lemmatizer.lemmatize(word.lower()) \\\n",
    "    for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+', ' ', str(x)))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_review_text</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not much to write about here but it doe exactl...</td>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the product doe exactly a it should and is qui...</td>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the primary job of this device is to block the...</td>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nice windscreen protects my mxl mic and preven...</td>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this pop filter is great it look and performs ...</td>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 cleaned_review_text  \\\n",
       "0  not much to write about here but it doe exactl...   \n",
       "1  the product doe exactly a it should and is qui...   \n",
       "2  the primary job of this device is to block the...   \n",
       "3  nice windscreen protects my mxl mic and preven...   \n",
       "4  this pop filter is great it look and performs ...   \n",
       "\n",
       "                                          reviewText  overall  \n",
       "0  Not much to write about here, but it does exac...        5  \n",
       "1  The product does exactly as it should and is q...        5  \n",
       "2  The primary job of this device is to block the...        5  \n",
       "3  Nice windscreen protects my MXL mic and preven...        5  \n",
       "4  This pop filter is great. It looks and perform...        5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data[['cleaned_review_text', 'reviewText', 'overall']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 计算 TF-IDF 向量\n",
    "\n",
    "计算清洗数据的 TF-IDF 并提取前 500 个关键词汇："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>12</th>\n",
       "      <th>20</th>\n",
       "      <th>34</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>accurate</th>\n",
       "      <th>acoustic</th>\n",
       "      <th>actually</th>\n",
       "      <th>...</th>\n",
       "      <th>won</th>\n",
       "      <th>work</th>\n",
       "      <th>worked</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067074</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079880</td>\n",
       "      <td>0.111989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.339573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.303608</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10  100   12   20   34      able     about  accurate  acoustic  actually  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.000000  0.159684       0.0       0.0       0.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000       0.0       0.0       0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000       0.0       0.0       0.0   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.339573  0.000000       0.0       0.0       0.0   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.000000  0.000000       0.0       0.0       0.0   \n",
       "\n",
       "   ...  won      work  worked  worth     would  wrong  year  yet       you  \\\n",
       "0  ...  0.0  0.134327     0.0    0.0  0.000000    0.0   0.0  0.0  0.000000   \n",
       "1  ...  0.0  0.085436     0.0    0.0  0.000000    0.0   0.0  0.0  0.067074   \n",
       "2  ...  0.0  0.000000     0.0    0.0  0.115312    0.0   0.0  0.0  0.079880   \n",
       "3  ...  0.0  0.000000     0.0    0.0  0.000000    0.0   0.0  0.0  0.000000   \n",
       "4  ...  0.0  0.000000     0.0    0.0  0.000000    0.0   0.0  0.0  0.303608   \n",
       "\n",
       "       your  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.111989  \n",
       "3  0.000000  \n",
       "4  0.000000  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_model = TfidfVectorizer(max_features=500)\n",
    "tfidf_df = pd.DataFrame(tfidf_model.fit_transform(review_data['cleaned_review_text']).todense())\n",
    "tfidf_df.columns = sorted(tfidf_model.vocabulary_)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 对标签进行编码\n",
    "\n",
    "由于原始文本是一个多分类样本，所以我们对其进行编码，创建一个新列 `target`，当总体参数 `overall`，即 `x<=4` 时编码为 `0`，否则编码为 `1`。\n",
    "\n",
    "这样，将多分类问题转换为了二分类问题，可以使用逻辑回归进行分类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6938\n",
       "0    3323\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data['target'] = review_data['overall'].apply(lambda x : 0 if x<=4 else 1)\n",
    "review_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 创建逻辑回归模型\n",
    "\n",
    "使用 tf-idf 向量和编码标签进行逻辑回归模型建立并进行预测得到概率。我们使用 scikit-learn 的 `LogisticRegression` 类创建基本模型。然后，利用 `fit()` 拟合函数对模型进行训练。然后，可以使用训练好的模型进行预测，另外，我们还可以使用 `predict_proba` 函数得到每个类的概率估计，示例代码如下：\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X,y)\n",
    "predicted_labels = log_reg.predict(X)\n",
    "predicted_probability = log_reg.predict_proba(X)[:,1]\n",
    "```\n",
    "\n",
    "这里，`X` 代表一个特征变量的 DataFrame，而 `y` 代表一个目标变量 (标签) 的 DataFrame。将示例代码套用到我们的实验中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57146961, 0.68579907, 0.56068939, ..., 0.65979968, 0.5495679 ,\n",
       "       0.21186011])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(tfidf_df, review_data['target']) # 指定标签列为新创建的 `target` 列\n",
    "predicted_labels = logreg.predict(tfidf_df)\n",
    "logreg.predict_proba(tfidf_df)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于逻辑回归的详细参数说明，[请查阅官方文档](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 创建交叉表格\n",
    "\n",
    "使用 pandas 的 `crosstab` 函数将分类模型的结果 (`predicted_labels`) 与商品评论的实际类 (`target`) 进行比较。此时的交叉表格相当于混淆矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predicted_labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1543</td>\n",
       "      <td>1780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>626</td>\n",
       "      <td>6312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predicted_labels     0     1\n",
       "target                      \n",
       "0                 1543  1780\n",
       "1                  626  6312"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data['predicted_labels'] = predicted_labels\n",
    "pd.crosstab(review_data['target'], review_data['predicted_labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们可以看到：\n",
    "\n",
    "- `1543` 个具有目标标签 `0` 的样本被正确分类，而 `1780` 个样本被错误分类。\n",
    "- `6312` 个具有目标标签 `1` 的样本被正确分类，而 `626` 个样本被错误分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验小结\n",
    "\n",
    "在本实验中，你实现了使用逻辑回归进行文本分类。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
